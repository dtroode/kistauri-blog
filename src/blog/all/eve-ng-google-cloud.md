---
title: "Установка EVE-NG на Google Cloud"
description: Полная установка EVE-NG на удалённый сервер Google Cloud, чтоб всё работало
date: 2021-10-19
tags: ["eve-ng", "виртуализация"]
---

Google Cloud подойдёт для студентов, он предоставляет бесплатные 300 долларов на 90 дней. Деньги тратятся на обслуживание сервера, затраты зависят от мощности сервера и частоты использования. Поэтому лучше выключать виртуальную машинку каждый раз, когда не используете её.

Плюсы Google Cloud в том, что не используются мощности вашего компьютера и получить полноценный доступ к Еве можно из любой точки с любого компьютера по айпи.

На главной странице выбираем список проектов и создаём новый проект.

![Нажимаем New Project](/images/eve-project.jpg)

## Готовим изображение

Ева обычно настраивается на Убунте, поэтому нужно подготовить подходящее изображение Убунты. Открываем внутреннюю консоль проекта.

![](/images/eve-internal-console.jpg)

Есть две версии Евы: Community и Pro. Вторая просит лицензионный ключ, первая бесплатная, обновляеся реже и в целом попроще. Для них нужны разные образы. Вставляем в консоль ту команду, которая соответствует будущей Еве.

Для Community:

```
gcloud compute images create nested-ubuntu-xenial --source-image-project=ubuntu-os-cloud --source-image-family=ubuntu-1604-lts --licenses="https://www.google.com/compute/v1/projects/vm-options/global/licenses/enable-vmx"
```

Для Pro:

```
gcloud compute images create nested-ubuntu-bionic --source-image-family=ubuntu-1804-lts --source-image-project=ubuntu-os-cloud --licenses https://www.googleapis.com/compute/v1/projects/vm-options/global/licenses/enable-vmx
```

## Виртуалка

Находясь в новом проекте, в поиске – находится наверху на любой странице – вбиваем VM Instances и переходим в раздел с виртуальными машинами. Создаём новую машину.

1. Выбираем имя.
2. Выбираем регион и зону. Нужен тот регион, который ближе к нам. Эта зона будет дальше часто использоваться, но запоминать её не надо, всегда можно посмотреть.
3. Настраиваем железо. Нам подойдёт процессор типа N1 или N2, характеристики процессора и оперативной памяти на своё усмотрение в заивисмости от проекта.

> Справа можно увидеть цену, которая уйдёт на обслуживание, но если не держать сервер включённым всё время, то она будет значительно меньше.

4. Выбирам загрузочный диск. Нажимаем изменить, выбираем опцию Custom images, указываем свой проект, указываем изображение, которое создали на прошлом шаге. Характеристики диска на своё усмотрение.
5. Ниже разрешаем HTTP траффик, потому что будем использовать Еву без SSL-сертификата. Натсройка HTTPS – отдельная тема.

Машинка создастся и скоро станет доступна

## Вложенная виртуализация

Вложенная виртуализация нужна, чтоб можно было запускать виртуальные машинки внутри Евы (которая тоже является виртуальной машинкой). По умолчанию она не настроена.

Выключаем нашу машинку.

![Выбираем Stop](/images/eve-on-off.jpg)

Открываем внутреннюю консоль проекта. Прописываем три команды, где меняем EVE_NAME на имя своей виртуальной машинки, а ZONE – на указанную зону этой машинки, её можно увидеть рядом с названием машинки.

Эта команда копирует конфиг машинки в файл YAML_FILE_PATH. Название файла можно изменить.

```
gcloud compute instances export EVE-NAME \
  --destination=YAML_FILE_PATH \
  --zone=ZONE
```

Эта команда добавляет в скопированный конфиг две строки, которые включают вложенную виртуализацию.

```
cat <<EOT >> YAML_FILE_PATH
advancedMachineFeatures:
  enableNestedVirtualization: true
EOT
```

Эта команда заменяет конфиг Евы на отредактированный нами, который хранится в файле.

```
gcloud compute instances update-from-file EVE-NAME \
  --source=YAML_FILE_PATH \
  --most-disruptive-allowed-action=RESTART \
  --zone=ZONE
```

Файл YAML_FILE_PATH можно удалять.

## Фаервол

По умолчанию многие порты блокируются виртуальной машинкой для внешнего доступа. Проблема в том, что доступ ко всем устройствам внутри лабораторных работ предоставляется по айпи Евы и порту устройства.

В поиске вбиваем Firewall, создаём правило для фаервола, придумываем ему имя. Разрешаем доступ по любому айпи к портам с 0 по 65535. Вот так:

![Остальное не меняем](/images/eve-firewall.jpg)

И создаём ещё одно такое же правило, где вместо Ingress ставим Egress. Теперь трафик может ходить в обе стороны. Надо понимать, что теперь любой человек сможет подключиться к устройствам внутри вашей лабораторки, зная внешний айпи Евы и порт устройства.

## Настройка интерфейсов

Машинка работает, но выхода в интернет у неё нет. Будем настраивать виртуальные интерфейсы таким образом, чтоб они передавали трафик на реальный интерфейс. Покажу на примере одного интерфейса. Всё делаем внутри Евы, чтоб зайти туда, используем кнопку SSH.

![Google Cloud попытается открыть ещё одно окно браузера, браузер может заблокировать это действие и сообщить об этом. В каждом браузере по-разному разрешается доступ, решение легко найти](/images/eve-console.jpg)

1.  Переходим к файлу с конфигами интерфейсов редактором nano: `nano /etc/network/interfaces`, здесь видим заголовок Cloud devices. Это всё интерфейсы. Настраиваем первый, чтоб он выглядел так:
    ```
    iface eth1 inet manual
    auto pnet1
    iface pnet1 inet static
        bridge_ports eth1
        bridge_stp off
        address 10.199.199.1
        netmask 255.255.255.0
    ```
    Можно указать другой адрес, он станет адресом для Cloud 1 внутри лабораторки. Чтоб сохранить файл и выйти Ctrl + o, Enter, Ctrl + x.
2.  Перезагружаем сетевой сервис `systemctl restart networking`. Теперь проверим, после ввода команды `ip a` у pnet1 должен появится айпи, который мы указали.
3.  Переходим к системному файлу: `nano /etc/sysctl.conf`, убираем решётку в строке `net.ipv4.ip_forward=1`, чтоб пакеты перенаправлялись. Так же сохраняем и выходим. Вводим команду `sysctl -p /etc/sysctl.conf`, чтоб применить настройки.
4.  Настриваем правила для исходящего трафика командой `iptables -t nat -A POSTROUTING -s 10.199.199.0/24 -o pnet0 -j MASQUERADE`. Здесь айпишник – сеть, которая должна соответствовать айпишнику интерфейса (указывали ранее).
5.  Командой `iptables -L -nv -t nat` можно проверить последнюю настройку, должны увидеть строку со знакомыми из прошлой команды словами. Чтоб сохранить изменения вводим несколько команд:
    ```
    iptables-save > /etc/iptables.rules
    nano /etc/network/if-pre-up.d/iptables
    ```
    Вставляем этот кусок кода, чтоб каждый раз правила сами применялись:
    ```
    #!/bin/sh
    iptables-restore < /etc/iptables.rules
    exit 0
    ```
    – и сохраняем Ctrl + o, Enter, Ctrl + x. Вводим команду:
    ```
    nano /etc/network/if-post-down.d/iptables
    ```
    Вставляем этот код, чтоб каждый раз правила сами сохранялись:
    ```
    #!/bin/sh
    iptables-save -c > /etc/iptables.rules
    if [ -f /etc/iptables.rules ]; then
        iptables-restore < /etc/iptables.rules
    fi
    exit 0
    ```
    Так же сохраняем и вводим две команды, чтоб сделать файлы доступными для запуска:
    ```
    sudo chmod +x /etc/network/if-post-down.d/iptables
    sudo chmod +x /etc/network/if-pre-up.d/iptables
    ```

Теперь сеть должна работать.

## Работа с Евой

В списке виртуальных машинок, нажав на имя машинки, можно увидеть и отредактировать её характеристики. Справа кнопка с тремя точками, чтоб запускать и выключать машинку.

External IP – адрес, по которому можно зайти в лабораторки Евы, Internal IP нужен для внутреннего использования, по нему тоже можно зайти в Еву, если натсроен SSH туннель, или из другой машинки.

В машинке пока почти нет образов, например Cisco, Mikrotik и разных операционок. Как их добавить расскажу отдельно.
